{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d014cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pandas.core.interchange.dataframe_protocol import DataFrame\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba2bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from coreforecast.rolling import rolling_mean\n",
    "from mlforecast.lag_transforms import RollingMean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from utilsforecast.plotting import plot_series\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from coreforecast.lag_transforms import ExpandingMean\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from utilsforecast.plotting import plot_series\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re\n",
    "\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.core import TimeSeries\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from neuralforecast.models import NHITS  # or RNN, TCN, NBEATS, etc.\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS\n",
    "from neuralforecast.models import RNN\n",
    "from neuralforecast.models import LSTM\n",
    "from neuralforecast.models import TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fee0f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5fe98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths\n",
    "data_path = \"/Users/adi.k/Desktop/Reichman/Advanced ML/Final project/Data/\"\n",
    "\n",
    "# Load CSVs\n",
    "train = pd.read_csv(data_path + \"train.csv\", parse_dates=[\"date\"])\n",
    "calendar = pd.read_csv(data_path + \"calendar_events.csv\", parse_dates=[\"date\"])\n",
    "submission = pd.read_csv(data_path + \"forecast_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff156f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, calendar, first_timestep = 0):\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'store_name': 'unique_id', 'date': 'ds', 'revenue': 'y'})\n",
    "\n",
    "    # Interpolate the zero-values (xmas)\n",
    "    if 'y' in df.columns:\n",
    "        df.loc[df['y'] == 0, 'y'] = np.nan\n",
    "        df['y'] = df.groupby('unique_id')['y'].transform(lambda s: s.interpolate(method='linear'))\n",
    "\n",
    "    # Timestep columns\n",
    "    df = df.sort_values('ds')\n",
    "    date_to_timestep = {date: i + first_timestep for i, date in enumerate(sorted(df['ds'].unique()))}\n",
    "    df['timestep'] = df['ds'].map(date_to_timestep)\n",
    "\n",
    "    # Merge with calendar events\n",
    "    df = df.merge(calendar.rename(columns={'date': 'ds'}), on='ds', how='left')\n",
    "    df['event'] = df['event'].fillna('None')\n",
    "\n",
    "    # Get all known event types (including 'None')\n",
    "    all_events = calendar['event'].dropna().unique().tolist()\n",
    "    all_events.append('None')\n",
    "\n",
    "    # Convert to categorical with fixed categories\n",
    "    df['event'] = pd.Categorical(df['event'], categories=all_events)\n",
    "\n",
    "    # One-hot encode\n",
    "    df = pd.get_dummies(df, columns=['event'], prefix='event')\n",
    "\n",
    "    # Sanitize column names\n",
    "    df.columns = [clean_column(col) for col in df.columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_column(name):\n",
    "    return re.sub(r'\\W+', '_', name)\n",
    "\n",
    "\n",
    "def plot_df(df, count = 4):\n",
    "    store_counts = df['unique_id'].value_counts()\n",
    "    top_stores = store_counts[store_counts.index != 'All Stores'].head(count).index.tolist()\n",
    "\n",
    "    # Plot revenue over time for top 4 stores\n",
    "    fig, axs = plt.subplots(len(top_stores), 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "    for i, store in enumerate(top_stores):\n",
    "        sns.lineplot(data=df[df['unique_id'] == store], x='ds', y='y', ax=axs[i])\n",
    "        axs[i].set_title(f'Store: {store}')\n",
    "        axs[i].set_ylabel('Revenue')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_forecasts(df_actual, df_forecast, models, store_ids=None, history_days=60):\n",
    "    \"\"\"\n",
    "    Plots actual vs forecasted revenue for selected stores.\n",
    "\n",
    "    Parameters:\n",
    "    - df_actual: DataFrame with actuals (`ds`, `y`, `unique_id`)\n",
    "    - df_forecast: DataFrame with forecast (`ds`, `y_pred`, `unique_id`)\n",
    "    - store_ids: list of store names to plot\n",
    "    - history_days: number of days of actual data to show\n",
    "    \"\"\"\n",
    "    if store_ids is None:\n",
    "        store_ids = df_forecast['unique_id'].unique()[:3]  # default to first 3 stores\n",
    "\n",
    "    fig, axs = plt.subplots(len(store_ids), 1, figsize=(12, 4 * len(store_ids)), sharex=True)\n",
    "\n",
    "    if len(store_ids) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, store_id in enumerate(store_ids):\n",
    "        ax = axs[i]\n",
    "        actual = df_actual[df_actual['unique_id'] == store_id].sort_values('ds')\n",
    "        forecast = df_forecast[df_forecast['unique_id'] == store_id].sort_values('ds')\n",
    "\n",
    "        ax.plot(actual['ds'].iloc[-history_days:], actual['y'].iloc[-history_days:], label='Actual', linewidth=2)\n",
    "\n",
    "        for model in models:\n",
    "            ax.plot(forecast['ds'], forecast[model], label=model, linestyle='--', marker='o')\n",
    "\n",
    "        ax.set_title(f'Forecast vs Actual for {store_id}')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def split_train_test(df, h):\n",
    "    last_dates = df.groupby('unique_id')['ds'].max().reset_index()\n",
    "    cutoff_dates = last_dates.copy()\n",
    "    cutoff_dates['cutoff'] = cutoff_dates['ds'] - pd.Timedelta(days=h)\n",
    "\n",
    "    df = df.merge(cutoff_dates[['unique_id', 'cutoff']], on='unique_id')\n",
    "    train_df = df[df['ds'] <= df['cutoff']].drop(columns='cutoff')\n",
    "    test_df = df[df['ds'] > df['cutoff']].drop(columns='cutoff')\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def calc_rmse(df_actual, df_forecast):\n",
    "    merged = df_actual.merge(df_forecast, on=['unique_id', 'ds'], how='left')\n",
    "    rmse = {}\n",
    "    for model in df_forecast.drop(columns=['unique_id', 'ds']).columns:\n",
    "        rmse[model] = np.sqrt(mean_squared_error(merged['y'], merged[model]))\n",
    "        print(f\"{model}: {rmse[model]:.2f}\")\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "579929f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 92  # number of days to predict\n",
    "df = preprocess(train, calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31984c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_recursive(nf: NeuralForecast, future: DataFrame, train_df: DataFrame, train_vars: list):\n",
    "    future = future.copy()\n",
    "    history = train_df.copy()\n",
    "    h = nf.h\n",
    "\n",
    "    # Ensure y exists in future\n",
    "    future['y'] = np.nan\n",
    "\n",
    "    # Useful vars for slicing and joining\n",
    "    vars_ext = train_vars + ['timestep']\n",
    "    start = int(future['timestep'].min())\n",
    "    end = int(future['timestep'].max())\n",
    "\n",
    "    preds = []\n",
    "    model_names = nf._get_model_names()\n",
    "\n",
    "    for i in range(start, end, h):\n",
    "        # 1. Predict the next h steps\n",
    "        pred = nf.predict(history)\n",
    "\n",
    "        # 2. Keep only model outputs + IDs\n",
    "        pred = pred[['unique_id', 'ds'] + model_names]\n",
    "\n",
    "        # 3. Save current predictions\n",
    "        preds.append(pred)\n",
    "\n",
    "        # 4. Merge with future to get exogenous info\n",
    "        merged = pred.merge(future, on=['unique_id', 'ds'], how='left')\n",
    "\n",
    "        # 5. Add mean prediction as fallback y\n",
    "        merged['y'] = pred[model_names].mean(axis=1)\n",
    "\n",
    "        # 6. Keep only necessary columns to feed next step\n",
    "        pred_next = merged[vars_ext]\n",
    "\n",
    "        # 7. Add to history\n",
    "        history = pd.concat([history, pred_next], ignore_index=True)[train_vars]\n",
    "\n",
    "    # Combine all step-wise predictions\n",
    "    full_preds = pd.concat(preds, ignore_index=True)\n",
    "\n",
    "    # Add averaged prediction column (optional)\n",
    "    full_preds['y_hat'] = full_preds[model_names].mean(axis=1)\n",
    "\n",
    "    return full_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52017c41",
   "metadata": {},
   "source": [
    "# Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8757e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exogenous features (event columns)\n",
    "exog_vars = [col for col in df.columns if col.startswith('event_')]\n",
    "test_vars = ['unique_id', 'ds'] + exog_vars\n",
    "train_vars = ['y'] + test_vars\n",
    "\n",
    "train_store_0 = df[df['store_id'] == 0].copy()\n",
    "train_others = df[df['store_id'] != 0].copy()\n",
    "train_others = train_others.sort_values(by=['store_id', 'ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b7eeb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_submission(df, calendar, train_df):\n",
    "    submission_df = df.copy()\n",
    "    submission_df[['store_id', 'ds']] = submission_df['id'].str.split('_', expand=True)\n",
    "    submission_df['store_id'] = submission_df['store_id'].astype(int)\n",
    "    submission_df['ds'] = pd.to_datetime(submission_df['ds'])\n",
    "    submission_df = submission_df.merge(store_id_map, on='store_id', how='left')\n",
    "    submission_df = preprocess(submission_df, calendar, max(train_df['timestep']) + 1)\n",
    "\n",
    "    return submission_df.sort_values(by=['store_id', 'ds']).copy()\n",
    "\n",
    "store_id_map = df[['store_id', 'unique_id']].drop_duplicates()\n",
    "\n",
    "submission_df = preprocess_submission(submission, calendar, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2860b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "313a62be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING ON FULL DATA FOR SUBMISSION ===\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1104\n",
      "[LightGBM] [Info] Number of data points in the train set: 16780, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 23840.687732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[CatBoostRegressor, LGBMRegressor, XGBRegressor, HistGradientBoostingRegressor], freq=D, lag_features=['lag1', 'lag7', 'lag14', 'lag28'], date_features=['dayofweek', 'month', 'is_month_end'], num_threads=4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Train Models on Full Dataset for Final Submission ===\n",
    "\n",
    "#-----------MLF-------------------\n",
    "\n",
    "print(\"\\n=== TRAINING ON FULL DATA FOR SUBMISSION ===\")\n",
    "\n",
    "# Define and configure the MLF model (using only CatBoost here)\n",
    "mlf_submission = MLForecast(\n",
    "    models=[\n",
    "        CatBoostRegressor(verbose=0),  # You can uncomment others if needed\n",
    "        lgb.LGBMRegressor(),\n",
    "        xgb.XGBRegressor(),\n",
    "        HistGradientBoostingRegressor()\n",
    "    ],\n",
    "    freq='D',\n",
    "    lags=[1, 7, 14, 28],  # Meaningful lags for 28-day horizon\n",
    "    date_features=['dayofweek', 'month', 'is_month_end'],\n",
    "    num_threads=4  # Parallel processing\n",
    ")\n",
    "\n",
    "\n",
    "# Train MLF on the entire training set (excluding store 0)\n",
    "mlf_submission.fit(df=train_others[train_vars], static_features=[])\n",
    "\n",
    "print(\"✅ MLForecast Models successfully trained on full dataset for submission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bea64dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING ON FULL DATA FOR SUBMISSION ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type          | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | loss         | MAE           | 0      | train\n",
      "1 | padder_train | ConstantPad1d | 0      | train\n",
      "2 | scaler       | TemporalNorm  | 0      | train\n",
      "3 | blocks       | ModuleList    | 3.4 M  | train\n",
      "-------------------------------------------------------\n",
      "3.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.4 M     Total params\n",
      "13.565    Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ebe3269c2d42a090e790a39d0ddb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models successfully trained on full dataset for submission.\n"
     ]
    }
   ],
   "source": [
    "# Define and configure the NeuralForecast model (NHITS)\n",
    "#-----------NF-------------------\n",
    "\n",
    "nf_submission = NeuralForecast(\n",
    "    models=[\n",
    "        NHITS(\n",
    "            input_size=4 * h,\n",
    "            h=h,\n",
    "            max_steps=1000,\n",
    "            scaler_type='standard',\n",
    "            val_check_steps=100\n",
    "        )\n",
    "    ],\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# Train NHITS model on the same full training data\n",
    "nf_submission.fit(df=train_others[train_vars])\n",
    "\n",
    "print(\"✅ NeuralForecast Model successfully trained on full dataset for submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd18dd8",
   "metadata": {},
   "source": [
    "# Predict Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a9d23",
   "metadata": {},
   "source": [
    "#### MLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5eca2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split submission into store 0 and other stores\n",
    "store0_df = submission_df[submission_df[\"store_id\"] == 0].copy()\n",
    "other_stores_df = submission_df[submission_df[\"store_id\"] != 0].copy()\n",
    "\n",
    "# Predict for all stores using MLF (to compute store0 as a sum)\n",
    "mlf_forecast_all = mlf_submission.predict(\n",
    "    h=submission_df[\"ds\"].nunique(),\n",
    "    X_df=other_stores_df[test_vars]\n",
    ")\n",
    "\n",
    "# Add store_id and unique_id back from the original input\n",
    "mlf_forecast_all = mlf_forecast_all.merge(\n",
    "    other_stores_df[[\"unique_id\", \"store_id\", \"ds\"]],\n",
    "    on=[\"unique_id\", \"ds\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5f6f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate synthetic store 0 forecast by summing predictions across stores per day\n",
    "store0_pred = mlf_forecast_all[mlf_forecast_all[\"store_id\"] != 0] \\\n",
    "    .groupby(\"ds\")[\"CatBoostRegressor\"].sum().reset_index()\n",
    "\n",
    "store0_pred[\"store_id\"] = 0\n",
    "store0_pred[\"unique_id\"] = \"All Stores\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9e3b917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with original store0 to get ID and metadata\n",
    "store0_pred = store0_df[[\"ds\", \"id\"]].merge(store0_pred, on=\"ds\", how=\"left\")\n",
    "# store0_pred = store0_pred.rename(columns={\"CatBoostRegressor\": \"prediction\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01f345",
   "metadata": {},
   "source": [
    "#### NF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81e2feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7820c24b3152407f9769afd1af5c696c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nf_forecast = predict_recursive(nf_submission, other_stores_df, train_others, train_vars)\n",
    "# nf_forecast = nf_forecast.rename(columns={\"NHITS\": \"prediction\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d05173c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge everything into a final submission DataFrame - only NN\n",
    "\n",
    "other_preds = nf_forecast[[\"unique_id\", \"ds\", \"y_hat\"]].merge(\n",
    "    other_stores_df[[\"unique_id\", \"ds\", \"id\",'store_id']],\n",
    "    on=[\"unique_id\", \"ds\"],\n",
    "    how=\"left\").sort_values(by=['store_id', 'ds']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70738f62",
   "metadata": {},
   "source": [
    "#### Final original method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "44ddbf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "      <th>store_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_20151001</td>\n",
       "      <td>258210.690076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_20151002</td>\n",
       "      <td>291962.499625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_20151003</td>\n",
       "      <td>356656.748823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_20151004</td>\n",
       "      <td>355721.087491</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_20151005</td>\n",
       "      <td>294549.695530</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>10_20151227</td>\n",
       "      <td>19863.480469</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>10_20151228</td>\n",
       "      <td>15937.326172</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>10_20151229</td>\n",
       "      <td>16156.546875</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>10_20151230</td>\n",
       "      <td>17472.130859</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>10_20151231</td>\n",
       "      <td>20361.652344</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1012 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     prediction  store_id\n",
       "0     0_20151001  258210.690076         0\n",
       "1     0_20151002  291962.499625         0\n",
       "2     0_20151003  356656.748823         0\n",
       "3     0_20151004  355721.087491         0\n",
       "4     0_20151005  294549.695530         0\n",
       "..           ...            ...       ...\n",
       "915  10_20151227   19863.480469        10\n",
       "916  10_20151228   15937.326172        10\n",
       "917  10_20151229   16156.546875        10\n",
       "918  10_20151230   17472.130859        10\n",
       "919  10_20151231   20361.652344        10\n",
       "\n",
       "[1012 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store0_pred = store0_pred.rename(columns={\"CatBoostRegressor\": \"prediction\"})\n",
    "other_preds = other_preds.rename(columns={\"y_hat\": \"prediction\"})\n",
    "\n",
    "final_submission = pd.concat([\n",
    "    store0_pred[[\"id\", \"prediction\",\"store_id\"]],\n",
    "    other_preds[[\"id\", \"prediction\",\"store_id\"]]])\n",
    "\n",
    "final_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e894eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 11 Christmas predictions to 0\n"
     ]
    }
   ],
   "source": [
    "# Extract date from 'id' (format is like \"0_20151225\")\n",
    "final_submission['ds'] = pd.to_datetime(final_submission['id'].str.split('_').str[1], format='%Y%m%d')\n",
    "\n",
    "# Set predictions on Christmas Day to 0\n",
    "christmas_mask = (final_submission['ds'].dt.month == 12) & (final_submission['ds'].dt.day == 25)\n",
    "final_submission.loc[christmas_mask, 'prediction'] = 0\n",
    "print(f\"Set {christmas_mask.sum()} Christmas predictions to 0\")\n",
    "\n",
    "# Ensure no negative predictions\n",
    "final_submission['prediction'] = final_submission['prediction'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c4e23724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final submission shape: (1012, 2)\n",
      "Prediction statistics:\n",
      "  Mean: 52818.82\n",
      "  Min: 0.00\n",
      "  Max: 376963.85\n",
      "  NaN values: 0\n",
      "\n",
      "Submission saved to 'final_submission.csv'\n",
      "\n",
      "Sample submission:\n",
      "           id  prediction\n",
      "0  0_20151001   258210.69\n",
      "1  0_20151002   291962.50\n",
      "2  0_20151003   356656.75\n",
      "3  0_20151004   355721.09\n",
      "4  0_20151005   294549.70\n",
      "5  0_20151006   264152.36\n",
      "6  0_20151007   259515.29\n",
      "7  0_20151008   260843.89\n",
      "8  0_20151009   293629.92\n",
      "9  0_20151010   365125.86\n"
     ]
    }
   ],
   "source": [
    "# Remove store_id and ds columns\n",
    "final_submission = final_submission.drop(['store_id', 'ds'], axis=1)\n",
    "final_submission[\"prediction\"] = final_submission[\"prediction\"].round(2)\n",
    "\n",
    "# Report stats\n",
    "print(f\"Final submission shape: {final_submission.shape}\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"  Mean: {final_submission['prediction'].mean():.2f}\")\n",
    "print(f\"  Min: {final_submission['prediction'].min():.2f}\")\n",
    "print(f\"  Max: {final_submission['prediction'].max():.2f}\")\n",
    "print(f\"  NaN values: {final_submission['prediction'].isna().sum()}\")\n",
    "\n",
    "# Save submission file\n",
    "final_submission.to_csv('final_submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved to 'final_submission.csv'\")\n",
    "\n",
    "# Show sample of submission\n",
    "print(f\"\\nSample submission:\")\n",
    "print(final_submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aa58e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='final_submission.csv' target='_blank'>final_submission.csv</a><br>"
      ],
      "text/plain": [
       "/Users/adi.k/final_submission.csv"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# This creates a clickable link in the notebook\n",
    "FileLink(\"final_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "67a459e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>NHITS</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>store_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>California – Sunset Plaza</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>30604.535156</td>\n",
       "      <td>30604.535156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>California – Sunset Plaza</td>\n",
       "      <td>2015-10-02</td>\n",
       "      <td>36917.597656</td>\n",
       "      <td>36917.597656</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>California – Sunset Plaza</td>\n",
       "      <td>2015-10-03</td>\n",
       "      <td>50028.234375</td>\n",
       "      <td>50028.234375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>California – Sunset Plaza</td>\n",
       "      <td>2015-10-04</td>\n",
       "      <td>52371.781250</td>\n",
       "      <td>52371.781250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>California – Sunset Plaza</td>\n",
       "      <td>2015-10-05</td>\n",
       "      <td>38112.125000</td>\n",
       "      <td>38112.125000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>Wisconsin – Badger Crossing</td>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>19863.480469</td>\n",
       "      <td>19863.480469</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>Wisconsin – Badger Crossing</td>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>15937.326172</td>\n",
       "      <td>15937.326172</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>Wisconsin – Badger Crossing</td>\n",
       "      <td>2015-12-29</td>\n",
       "      <td>16156.546875</td>\n",
       "      <td>16156.546875</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Wisconsin – Badger Crossing</td>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>17472.130859</td>\n",
       "      <td>17472.130859</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>Wisconsin – Badger Crossing</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>20361.652344</td>\n",
       "      <td>20361.652344</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>920 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       unique_id         ds         NHITS         y_hat   \n",
       "276    California – Sunset Plaza 2015-10-01  30604.535156  30604.535156  \\\n",
       "277    California – Sunset Plaza 2015-10-02  36917.597656  36917.597656   \n",
       "278    California – Sunset Plaza 2015-10-03  50028.234375  50028.234375   \n",
       "279    California – Sunset Plaza 2015-10-04  52371.781250  52371.781250   \n",
       "280    California – Sunset Plaza 2015-10-05  38112.125000  38112.125000   \n",
       "..                           ...        ...           ...           ...   \n",
       "731  Wisconsin – Badger Crossing 2015-12-27  19863.480469  19863.480469   \n",
       "732  Wisconsin – Badger Crossing 2015-12-28  15937.326172  15937.326172   \n",
       "733  Wisconsin – Badger Crossing 2015-12-29  16156.546875  16156.546875   \n",
       "734  Wisconsin – Badger Crossing 2015-12-30  17472.130859  17472.130859   \n",
       "735  Wisconsin – Badger Crossing 2015-12-31  20361.652344  20361.652344   \n",
       "\n",
       "     store_id  \n",
       "276         1  \n",
       "277         1  \n",
       "278         1  \n",
       "279         1  \n",
       "280         1  \n",
       "..        ...  \n",
       "731        10  \n",
       "732        10  \n",
       "733        10  \n",
       "734        10  \n",
       "735        10  \n",
       "\n",
       "[920 rows x 5 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed2d94",
   "metadata": {},
   "source": [
    "# Post Processing - Bias trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6bb109",
   "metadata": {},
   "source": [
    "We modify the overall prediction according to the bias ratio, calculated by the revenue avg in oct-dec from train to adjust the submission predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about monthes from train\n",
    "\n",
    "train_others[\"month\"] = train_others[\"ds\"].dt.month\n",
    "seasonal_train = train_others[train_others[\"month\"].isin([10, 11, 12])]\n",
    "\n",
    "train_monthly_avg = seasonal_train.groupby(\"month\")[\"y\"].mean()\n",
    "train_monthly_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e37180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "final_submission[\"ds\"] = pd.to_datetime(final_submission[\"id\"].str.split(\"_\").str[1])\n",
    "\n",
    "# Extract store ID\n",
    "# final_submission[\"store_id\"] = final_submission[\"id\"].str.split(\"_\").str[0].astype(int)\n",
    "\n",
    "# Keep only individual stores (exclude store 0)\n",
    "individual_store_preds = final_submission[final_submission[\"store_id\"] != 0].copy()\n",
    "\n",
    "# Add month column\n",
    "individual_store_preds[\"month\"] = individual_store_preds[\"ds\"].dt.month\n",
    "\n",
    "# Filter to October, November, December\n",
    "seasonal_pred = individual_store_preds[individual_store_preds[\"month\"].isin([10, 11, 12])]\n",
    "\n",
    "# Compute average predictions per month\n",
    "pred_monthly_avg = seasonal_pred.groupby(\"month\")[\"prediction\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_ratio = train_monthly_avg / pred_monthly_avg\n",
    "print(bias_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map month to correction factor\n",
    "correction_map = bias_ratio.to_dict()\n",
    "\n",
    "# Apply correction only to individual stores\n",
    "# final_submission[\"store_id\"] = final_submission[\"id\"].str.split(\"_\").str[0].astype(int)\n",
    "# final_submission[\"ds\"] = pd.to_datetime(final_submission[\"id\"].str.split(\"_\").str[1])\n",
    "final_submission[\"month\"] = final_submission[\"ds\"].dt.month\n",
    "\n",
    "# Apply correction only to store_id != 0 and months 10–12\n",
    "final_submission[\"prediction_corrected\"] = final_submission.apply(\n",
    "    lambda row: row[\"prediction\"] * correction_map.get(row[\"month\"], 1.0)\n",
    "    if row[\"store_id\"] != 0 else row[\"prediction\"],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the corrected predictions exist\n",
    "# assert \"prediction_corrected\" in final_submission.columns, \"Missing corrected predictions!\"\n",
    "\n",
    "# Convert id into components\n",
    "# final_submission[\"store_id\"] = final_submission[\"id\"].str.split(\"_\").str[0].astype(int)\n",
    "# final_submission[\"ds\"] = pd.to_datetime(final_submission[\"id\"].str.split(\"_\").str[1])\n",
    "\n",
    "# Filter only real stores (excluding store 0)\n",
    "real_store_preds = final_submission[final_submission[\"store_id\"] != 0].copy()\n",
    "\n",
    "store0_updated = real_store_preds.groupby(\"ds\")[\"prediction_corrected\"].sum().reset_index()\n",
    "store0_updated[\"store_id\"] = 0\n",
    "store0_updated[\"id\"] = store0_updated[\"store_id\"].astype(str) + \"_\" + store0_updated[\"ds\"].dt.strftime(\"%Y%m%d\")\n",
    ".sort_values(by=['store_id', 'ds'])\n",
    "\n",
    "# Drop old store 0 entries\n",
    "final_submission = final_submission[final_submission[\"store_id\"] != 0].copy()\n",
    "\n",
    "# Create the final, merged submission\n",
    "store0_updated = store0_updated.rename(columns={\"prediction_corrected\": \"prediction\"})\n",
    "final_submission = pd.concat([\n",
    "    final_submission[[\"id\", \"prediction_corrected\"]].rename(columns={\"prediction_corrected\": \"prediction\"}),\n",
    "    store0_updated[[\"id\", \"prediction\"]]\n",
    "], axis=0).sort_values(\"id\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4311a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission[\"store_id\"] = final_submission[\"id\"].str.split(\"_\").str[0].astype(int)\n",
    "final_submission[\"ds\"] = pd.to_datetime(final_submission[\"id\"].str.split(\"_\").str[1])\n",
    "final_submission = final_submission.sort_values(by=['store_id', 'ds']).copy()\n",
    "final_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949264fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Extract date from 'id' (format is like \"0_20151225\")\n",
    "# final_submission['ds'] = pd.to_datetime(final_submission['id'].str.split('_').str[1], format='%Y%m%d')\n",
    "\n",
    "# Set predictions on Christmas Day to 0\n",
    "christmas_mask = (final_submission['ds'].dt.month == 12) & (final_submission['ds'].dt.day == 25)\n",
    "final_submission.loc[christmas_mask, 'prediction'] = 0\n",
    "\n",
    "print(f\"Set {christmas_mask.sum()} Christmas predictions to 0\")\n",
    "\n",
    "# Ensure no negative predictions\n",
    "final_submission['prediction'] = final_submission['prediction'].clip(lower=0)\n",
    "final_submission[\"prediction\"] = final_submission[\"prediction\"].round(2).copy()\n",
    "\n",
    "# Drop temporary 'ds' column if you don’t want it in the CSV\n",
    "final_submission = final_submission.drop(columns=['ds','store_id'])\n",
    "\n",
    "# Report stats\n",
    "print(f\"Final submission shape: {final_submission.shape}\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"  Mean: {final_submission['prediction'].mean():.2f}\")\n",
    "print(f\"  Min: {final_submission['prediction'].min():.2f}\")\n",
    "print(f\"  Max: {final_submission['prediction'].max():.2f}\")\n",
    "print(f\"  NaN values: {final_submission['prediction'].isna().sum()}\")\n",
    "\n",
    "# Save submission file\n",
    "final_submission.to_csv('final_submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved to 'final_submission.csv'\")\n",
    "\n",
    "# Show sample of submission\n",
    "print(f\"\\nSample submission:\")\n",
    "print(final_submission.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85313f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Save submission file\n",
    "final_submission.to_csv('final_submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved to 'final_submission.csv'\")\n",
    "\n",
    "# This creates a clickable link in the notebook\n",
    "FileLink(\"final_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561a34b",
   "metadata": {},
   "source": [
    "# Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbdbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping dictionary\n",
    "store_mapping = {\n",
    "   'California – Sunset Plaza': 1,\n",
    "   'California – Ocean View': 2,\n",
    "   'California – Golden Hills': 3,\n",
    "   'California – Redwood Center': 4,\n",
    "   'Texas – Lone Star Mall': 5,\n",
    "   'Texas – Riverwalk Market': 6,\n",
    "   'Texas – Alamo Heights': 7,\n",
    "   'Wisconsin – Maple Grove': 8,\n",
    "   'Wisconsin – Lakeview Plaza': 9,\n",
    "   'Wisconsin – Badger Crossing': 10\n",
    "}\n",
    "\n",
    "# Ensemble method\n",
    "mlf_forecast_all['store_id'] = mlf_forecast_all['unique_id'].map(store_mapping)\n",
    "nf_forecast['store_id'] = nf_forecast['unique_id'].map(store_mapping)\n",
    "\n",
    "mlf_forecast_all=mlf_forecast_all.sort_values(by=['store_id', 'ds'])\n",
    "nf_forecast=nf_forecast.sort_values(by=['store_id', 'ds'])\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "def create_results_df(mlf_forecast, nf_forecast):\n",
    "   # Merge on index\n",
    "   merged_df = mlf_forecast.merge(nf_forecast[['NHITS']], left_index=True, right_index=True)\n",
    "   \n",
    "   return merged_df\n",
    "\n",
    "# merged all models predictions\n",
    "results = create_results_df(mlf_forecast_all, nf_forecast)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "\n",
    "# Create weighted preidciton using diffrent models outputs\n",
    "class StoreSpecificEnsemble:\n",
    "   def __init__(self, strategy='inverse_rmse'):\n",
    "       self.strategy = strategy\n",
    "       self.models = ['CatBoostRegressor', 'HistGradientBoostingRegressor', \n",
    "                     'LGBMRegressor', 'XGBRegressor', 'NHITS']\n",
    "       \n",
    "       rmse_data = {\n",
    "           1: [2474.43, 2389.84, 2398.40, 2495.82, 2738.14],\n",
    "           2: [2256.06, 2334.11, 2085.16, 2415.22, 3217.97],\n",
    "           3: [3755.46, 3871.92, 3991.23, 4379.58, 4342.67],\n",
    "           4: [1929.34, 2007.67, 1783.69, 2111.14, 1606.26],\n",
    "           5: [2010.15, 2247.20, 2156.13, 1864.35, 2107.06],\n",
    "           6: [2539.36, 2329.24, 2384.43, 2380.53, 2912.74],\n",
    "           7: [2249.64, 2059.98, 2171.54, 2119.05, 2585.45],\n",
    "           8: [2072.15, 1641.65, 1987.03, 1855.08, 2251.23],\n",
    "           9: [3847.61, 3662.71, 3930.24, 3865.09, 2375.04],\n",
    "           10: [1775.24, 1600.33, 1723.86, 1881.33, 1899.07]\n",
    "       }\n",
    "       \n",
    "       self.store_weights = {}\n",
    "       for store_id, rmse_vals in rmse_data.items():\n",
    "           if strategy == 'inverse_rmse':\n",
    "               inverse_rmse = 1 / np.array(rmse_vals)\n",
    "               self.store_weights[store_id] = inverse_rmse / inverse_rmse.sum()\n",
    "           elif strategy == 'equal':\n",
    "               self.store_weights[store_id] = [0.2] * 5\n",
    "           elif strategy == 'best_only':\n",
    "               best_idx = np.argmin(rmse_vals)\n",
    "               weights = [0] * 5\n",
    "               weights[best_idx] = 1.0\n",
    "               self.store_weights[store_id] = weights\n",
    "   \n",
    "   \n",
    "    def predict(self, predictions_dict, store_ids):\n",
    "       ensemble_preds = np.zeros(len(store_ids))\n",
    "       \n",
    "       for i, store_id in enumerate(store_ids):\n",
    "           weights = self.store_weights.get(store_id, [0.2] * 5)\n",
    "           pred_sum = sum(weights[j] * predictions_dict[model][i] \n",
    "                         for j, model in enumerate(self.models) \n",
    "                         if model in predictions_dict)\n",
    "           ensemble_preds[i] = pred_sum\n",
    "       \n",
    "       return ensemble_preds\n",
    "\n",
    "    \n",
    "    \n",
    "#---------------------------------------\n",
    "    \n",
    "# Create ensemble\n",
    "ensemble = StoreSpecificEnsemble('best_only') \n",
    "\n",
    "# Extract predictions from results_df\n",
    "predictions_dict = {\n",
    "   'CatBoostRegressor': results['CatBoostRegressor'].values,\n",
    "   'HistGradientBoostingRegressor': results['HistGradientBoostingRegressor'].values,\n",
    "   'LGBMRegressor': results['LGBMRegressor'].values,\n",
    "   'XGBRegressor': results['XGBRegressor'].values,\n",
    "   'NHITS': results['NHITS'].values\n",
    "}\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "# Get ensemble predictions\n",
    "results['prediction'] = ensemble.predict(predictions_dict, results['store_id'].values)\n",
    "\n",
    "# Group by date and sum HistGradientBoostingRegressor predictions across all stores\n",
    "summary_df = results.groupby('ds')['HistGradientBoostingRegressor'].sum().reset_index()\n",
    "summary_df['store_id'] = 0\n",
    "summary_df = summary_df.rename(columns={'HistGradientBoostingRegressor': 'prediction'})\n",
    "summary_df = summary_df[['ds', 'store_id', 'prediction']]\n",
    "\n",
    "final_submission = pd.concat([\n",
    "    summary_df[[\"ds\", \"prediction\",\"store_id\"]],\n",
    "    results[[\"ds\", \"prediction\",\"store_id\"]]\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "final_submission = final_submission.sort_values(by=['store_id', 'ds']).copy()\n",
    "\n",
    "# Set predictions on Christmas Day to 0\n",
    "christmas_mask = (final_submission['ds'].dt.month == 12) & (final_submission['ds'].dt.day == 25)\n",
    "final_submission.loc[christmas_mask, 'prediction'] = 0\n",
    "print(f\"Set {christmas_mask.sum()} Christmas predictions to 0\")\n",
    "\n",
    "# Ensure no negative predictions\n",
    "final_submission['prediction'] = final_submission['prediction'].clip(lower=0)\n",
    "final_submission[\"prediction\"] = final_submission[\"prediction\"].round(2)\n",
    "\n",
    "# Create id column\n",
    "final_submission['id'] = final_submission['store_id'].astype(str) + '_' + pd.to_datetime(final_submission['ds']).dt.strftime('%Y%m%d')\n",
    "\n",
    "# Remove store_id and ds columns\n",
    "final_submission = final_submission.drop(['store_id', 'ds'], axis=1)\n",
    "\n",
    "final_submission = final_submission[['id', 'prediction']]\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Save submission file\n",
    "final_submission.to_csv('final_submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved to 'final_submission.csv'\")\n",
    "\n",
    "# This creates a clickable link in the notebook\n",
    "FileLink(\"final_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
